{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "import sqlite3\n",
    "import json\n",
    "import re\n",
    "\n",
    "from shutil import copyfile\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Number of facial landmarks provided by AFLW dataset\n",
    "N_LANDMARK = 21"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_size(image_filename):\n",
    "    im = Image.open(image_filename)\n",
    "    return im.size[0], im.size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exec_sqlite_query(cursor, select_str, from_str=None, where_str=None):\n",
    "    query_str = 'SELECT {}'.format(select_str)\n",
    "    query_str += ' FROM {}'.format(from_str)\n",
    "    if where_str:\n",
    "        query_str += ' WHERE {}'.format(where_str)\n",
    "    return [row for row in cursor.execute(query_str)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def progress_updt(msg, total, progress):\n",
    "    bar_length, status = 20, \"\"\n",
    "    progress = float(progress) / float(total)\n",
    "    if progress >= 1.:\n",
    "        progress, status = 1, \"\\r\\n\"\n",
    "    block = int(round(bar_length * progress))\n",
    "    text = \"\\r{}[{}] {:.0f}% {}\".format(msg, \"#\" * block + \"-\" * (bar_length - block), round(progress * 100, 0), status)\n",
    "    sys.stdout.write(text)\n",
    "    sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rgb_list_21 = [[255, 0, 0],\n",
    "               [178, 34, 34],\n",
    "               [255, 20, 147],\n",
    "               [199, 21, 133],\n",
    "               [255, 69, 0],\n",
    "               [255, 255, 0],\n",
    "               [255, 0, 255],\n",
    "               [128, 0, 128],\n",
    "               [75, 0, 130],\n",
    "               [218, 165, 32],\n",
    "               [173, 255, 47],\n",
    "               [0, 255, 0],\n",
    "               [152, 251, 152],\n",
    "               [0, 128, 0],\n",
    "               [154, 205, 50],\n",
    "               [0, 255, 255],\n",
    "               [127, 255, 212],\n",
    "               [72, 209, 204],\n",
    "               [70, 130, 180],\n",
    "               [0, 191, 255],\n",
    "               [30, 144, 255],\n",
    "               [30, 145, 255]] # +1 face\n",
    "len(rgb_list_21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_box = 6\n",
    "\n",
    "def main():\n",
    "    # Set up a parser for command line arguments\n",
    "    isVerbose = 1\n",
    "    dataset_root = \"/Users/nikitakorobkov/Downloads/aflw/data\"\n",
    "    jsonFile = \"panoptic_aflwcoco8.json\"\n",
    "    dirForNewJPGs = \"allImagesJPG3/\"\n",
    "    dirForNewPNGs = \"allImagesPNG3/\"\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"#. Transform AFLW annotations into COCO json format...\")\n",
    "\n",
    "    # Open the original AFLW annotation (sqlite database)\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Open the AFLW SQLight database...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    conn = sqlite3.connect(osp.join(dataset_root, 'aflw.sqlite'))\n",
    "    cursor = conn.cursor()\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Build sqlite queries\n",
    "    select_str = \"faces.face_id, \" \\\n",
    "                 \"imgs.filepath, \" \\\n",
    "                 \"rect.x, rect.y, \" \\\n",
    "                 \"rect.w, \" \\\n",
    "                 \"rect.h, \" \\\n",
    "                 \"pose.roll, \" \\\n",
    "                 \"pose.pitch, \" \\\n",
    "                 \"pose.yaw, \" \\\n",
    "                 \"metadata.sex\"\n",
    "    from_str = \"faces, \" \\\n",
    "               \"faceimages \" \\\n",
    "               \"imgs, \" \\\n",
    "               \"facerect rect, \" \\\n",
    "               \"facepose pose, \" \\\n",
    "               \"facemetadata metadata\"\n",
    "    where_str = \"faces.file_id = imgs.file_id and \" \\\n",
    "                \"faces.face_id = rect.face_id and \" \\\n",
    "                \"faces.face_id = pose.face_id and \" \\\n",
    "                \"faces.face_id = metadata.face_id\"\n",
    "    query_res = exec_sqlite_query(cursor, select_str, from_str, where_str)\n",
    "\n",
    "    # Count total number of images in AFLW dataset\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Count total number of images in AFLW database: \", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    total_num_images = 0\n",
    "    for _ in query_res:\n",
    "        total_num_images += 1\n",
    "    if isVerbose:\n",
    "        print(total_num_images)\n",
    "\n",
    "    # Output file for appending the file paths of not found images\n",
    "    not_found_images_file = 'not_found_images_aflw.txt'\n",
    "    try:\n",
    "        os.remove(not_found_images_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Temporary dataset variables\n",
    "    aflw_dataset_dict = dict()\n",
    "\n",
    "    # Register to dataset_dict\n",
    "    img_cnt = 0\n",
    "    for face_id, path, rectx, recty, rectw, recth, roll, pitch, yaw, gender in query_res:\n",
    "\n",
    "        img_cnt += 1\n",
    "\n",
    "        # Get current image path\n",
    "        img_path = osp.join(dataset_root, 'flickr', path)\n",
    "\n",
    "        # Process current image\n",
    "        if osp.isfile(img_path):\n",
    "            img_w, img_h = get_img_size(img_path)\n",
    "\n",
    "            keypoints = N_LANDMARK * 3 * [0]\n",
    "            pose = [roll, pitch, yaw]\n",
    "            gender = 0 if gender == 'm' else 1\n",
    "\n",
    "            # Register\n",
    "            aflw_dataset_dict[face_id] = {\n",
    "                'face_id': face_id,\n",
    "                'img_path': osp.join('flickr', path),\n",
    "                'width': img_w,\n",
    "                'height': img_h,\n",
    "                'bbox': (rectx, recty, rectw, recth),\n",
    "                'keypoints': keypoints,\n",
    "                'pose': pose,\n",
    "                'gender': gender}\n",
    "\n",
    "        # If current image file does not exist append the not found images filepaths to `not_found_images_file` and\n",
    "        # continue with the next image file.\n",
    "        else:\n",
    "            with open(not_found_images_file, \"a\") as out:\n",
    "                out.write(\"%s\\n\" % img_path)\n",
    "            continue\n",
    "\n",
    "        # Show progress bar\n",
    "        if isVerbose:\n",
    "            progress_updt(\"  \\\\__Populate AFLW dataset dictionary...\", total_num_images, img_cnt)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Update AFLW dataset dictionary with keypoints...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Landmark property\n",
    "    # (Visibility is expressed by lack of the coordinate's row.)\n",
    "    select_str = \"faces.face_id, coords.feature_id, coords.x, coords.y\"\n",
    "    from_str = \"faces, featurecoords coords\"\n",
    "    where_str = \"faces.face_id = coords.face_id\"\n",
    "    query_res = exec_sqlite_query(cursor, select_str, from_str, where_str)\n",
    "\n",
    "    # Register to dataset_dict\n",
    "    invalid_face_ids = list()\n",
    "    for face_id, feature_id, x, y in query_res:\n",
    "        assert (1 <= feature_id <= N_LANDMARK)\n",
    "        if face_id in aflw_dataset_dict:\n",
    "            idx = feature_id - 1\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx] = x\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx + 1] = y\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx + 2] = 1\n",
    "\n",
    "        elif face_id not in invalid_face_ids:\n",
    "            invalid_face_ids.append(face_id)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Close database\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Close the AFLW SQLight database...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    cursor.close()\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Close database\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Convert to COCO format...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    images_list = []\n",
    "    annotations_list = []\n",
    "    iterI = 0\n",
    "    for face_id, face_ann in aflw_dataset_dict.items():\n",
    "        if iterI > 9998:\n",
    "            break\n",
    "        iterI = iterI + 1\n",
    "        \n",
    "        img_dir_num = int(face_ann['img_path'].split(\"/\")[1])\n",
    "        img_file_num = int(re.findall(r'\\d+', face_ann['img_path'].split(\"/\")[-1].split(\".\")[0])[0])\n",
    "        image_id = int(\"%d%05d\" % (img_dir_num, img_file_num))\n",
    "\n",
    "        #save jpg\n",
    "        new_file_name = face_ann['img_path'].split(\"/\")[2]\n",
    "        if(new_file_name.split(\".\")[1] == \"png\"):\n",
    "            new_file_name = new_file_name.split(\".\")[0] + \".jpg\"\n",
    "            \n",
    "        new_path_file = \"allImagesJPG3/\" + new_file_name\n",
    "        copyfile(face_ann['img_path'], new_path_file)\n",
    "        \n",
    "        images_list.append({'id': image_id,\n",
    "                            'file_name': new_file_name,\n",
    "                            'height': face_ann['height'],\n",
    "                            'width': face_ann['width'],\n",
    "                            'date_captured': '',\n",
    "                            'flickr_url': '',\n",
    "                            'license': 1,\n",
    "                            'dataset': 'aflw'})\n",
    "\n",
    "        list_keypoints = []\n",
    "        one_keypoint = []\n",
    "        for value in face_ann['keypoints']:\n",
    "            one_keypoint.append(value)\n",
    "            if len(one_keypoint)==3:\n",
    "                list_keypoints.append(one_keypoint)\n",
    "                one_keypoint= []\n",
    "\n",
    "        #create Array2PNG\n",
    "        w, h = face_ann['width'], face_ann['height']\n",
    "        array2png = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        \n",
    "        segmentations_list = []\n",
    "        for i, keypoint in enumerate(list_keypoints):\n",
    "            v=keypoint[2]\n",
    "            if v == 1: #видно keypoint\n",
    "                x=int(keypoint[0])\n",
    "                y=int(keypoint[1])\n",
    "                segmentation_array = [x,y,x+size_box,y,x+size_box,y+size_box,x,y+size_box]\n",
    "                category_id = i #номер категории[0-20]\n",
    "                id_segmentation = rgb_list_21[i][0] + rgb_list_21[i][1]*256 + rgb_list_21[i][2]*256*256 #мапинг с PNG через RGB: ids=R+G*256+B*256^2\n",
    "                segmentations_list.append({'id': id_segmentation,\n",
    "                                           'category_id': category_id,\n",
    "                                           'area':0,\n",
    "                                           'bbox':[x,y,size_box,size_box],\n",
    "                                           'bbox_mode': \"<BoxMode.XYWH_ABS: 1>\",\n",
    "                                           'iscrowd':0,\n",
    "                                           'segmentation':segmentation_array})\n",
    "                #update array2png\n",
    "                array2png[y:y+size_box, x:x+size_box] = rgb_list_21[i]\n",
    "        \n",
    "        #create PNG\n",
    "        img = Image.fromarray(array2png, 'RGB')\n",
    "        path2origFile = face_ann['img_path']\n",
    "        path2origFile = path2origFile.split(\"/\")\n",
    "        new_path_file_name = dirForNewPNGs + path2origFile[2].split(\".\")[0] + '.png'\n",
    "        new_file_name = path2origFile[2].split(\".\")[0] + '.png'\n",
    "        img.save(new_path_file_name)\n",
    "        \n",
    "        annotations_list.append({'image_id': image_id,\n",
    "                                 'file_name': new_file_name, #PNG file name,\n",
    "                                 'segments_info': segmentations_list})\n",
    "                                 #'num_keypoints': len(face_ann['keypoints']),\n",
    "                                 #'area': 0,\n",
    "                                 #'iscrowd': 0,\n",
    "                                 #'keypoints': face_ann['keypoints'],\n",
    "                                 #'bbox': face_ann['bbox'],\n",
    "                                 #'category_id': 0})\n",
    "\n",
    "    # Build COCO-like dictionary\n",
    "    dataset_dict = dict()\n",
    "\n",
    "    # =============================== Dataset Info =============================== #\n",
    "    dataset_info = {\n",
    "        'description': 'Annotated Facial Landmarks in the Wild (AFLW)',\n",
    "        'url': 'https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/',\n",
    "        'version': '1.0',\n",
    "        'year': 2011,\n",
    "        'contributor': '',\n",
    "        'date_created': '2011'\n",
    "    }\n",
    "    dataset_dict.update(dataset_info)\n",
    "\n",
    "    # ============================= Dataset Licenses ============================= #\n",
    "    dataset_licenses = {\n",
    "        'licenses': [\n",
    "            {'id': 0,\n",
    "             'url': 'https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/',\n",
    "             'name': 'aflw_license'}\n",
    "        ]\n",
    "    }\n",
    "    dataset_dict.update(dataset_licenses)\n",
    "\n",
    "    # ============================== Dataset Images ============================== #\n",
    "    dataset_images = {'images': images_list}\n",
    "    dataset_dict.update(dataset_images)\n",
    "\n",
    "    # =========================== Dataset Annotations ============================ #\n",
    "    dataset_annotations = {'annotations': annotations_list}\n",
    "    dataset_dict.update(dataset_annotations)\n",
    "\n",
    "    # ============================ Dataset Categories ============================ #\n",
    "    all_keypoints = ['LeftBrowLeftCorner',\n",
    "                     'LeftBrowCenter',\n",
    "                     'LeftBrowRightCorner',\n",
    "                     'RightBrowLeftCorner',\n",
    "                     'RightBrowCenter',\n",
    "                     'RightBrowRightCorner',\n",
    "                     'LeftEyeLeftCorner',\n",
    "                     'LeftEyeCenter',\n",
    "                     'LeftEyeRightCorner',\n",
    "                     'RightEyeLeftCorner',\n",
    "                     'RightEyeCenter',\n",
    "                     'RightEyeRightCorner',\n",
    "                     'LeftEar',\n",
    "                     'NoseLeft',\n",
    "                     'NoseCenter',\n",
    "                     'NoseRight',\n",
    "                     'RightEar',\n",
    "                     'MouthLeftCorner',\n",
    "                     'MouthCenter',\n",
    "                     'MouthRightCorner',\n",
    "                     'ChinCenter']\n",
    "    list_categories = []\n",
    "    for i, category in enumerate(all_keypoints):\n",
    "        list_categories.append({'id': i,\n",
    "                                'name': category,\n",
    "                                'supercategory': category,\n",
    "                                'isthing': 0, #если не отображается сменить на [0,1]\n",
    "                                'color': rgb_list_21[i]})\n",
    "    dataset_categories = {'categories': list_categories}\n",
    "    dataset_dict.update(dataset_categories)\n",
    "\n",
    "    \n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Save dataset dictionary as json file\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Save dataset dictionary as json file...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    with open(jsonFile, 'w') as fp:\n",
    "        json.dump(dataset_dict, fp)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#. Transform AFLW annotations into COCO json format...\n",
      "  \\__Open the AFLW SQLight database...Done!\n",
      "  \\__Count total number of images in AFLW database: 24384\n",
      "  \\__Populate AFLW dataset dictionary...[####################] 100% \n",
      "  \\__Update AFLW dataset dictionary with keypoints...Done!\n",
      "  \\__Close the AFLW SQLight database...Done!\n",
      "  \\__Convert to COCO format...Done!\n",
      "  \\__Save dataset dictionary as json file...Done!\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_box = 6\n",
    "\n",
    "def main_instance():\n",
    "    # Set up a parser for command line arguments\n",
    "    isVerbose = 1\n",
    "    dataset_root = \"/Users/nikitakorobkov/Downloads/aflw/data\"\n",
    "    jsonFile = \"panoptic_aflwcoco8.json\"\n",
    "    dirForNewJPGs = \"allImagesJPG3/\"\n",
    "    dirForNewPNGs = \"allImagesPNG3/\"\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"#. Transform AFLW annotations into COCO json format...\")\n",
    "\n",
    "    # Open the original AFLW annotation (sqlite database)\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Open the AFLW SQLight database...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    conn = sqlite3.connect(osp.join(dataset_root, 'aflw.sqlite'))\n",
    "    cursor = conn.cursor()\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Build sqlite queries\n",
    "    select_str = \"faces.face_id, \" \\\n",
    "                 \"imgs.filepath, \" \\\n",
    "                 \"rect.x, rect.y, \" \\\n",
    "                 \"rect.w, \" \\\n",
    "                 \"rect.h, \" \\\n",
    "                 \"pose.roll, \" \\\n",
    "                 \"pose.pitch, \" \\\n",
    "                 \"pose.yaw, \" \\\n",
    "                 \"metadata.sex\"\n",
    "    from_str = \"faces, \" \\\n",
    "               \"faceimages \" \\\n",
    "               \"imgs, \" \\\n",
    "               \"facerect rect, \" \\\n",
    "               \"facepose pose, \" \\\n",
    "               \"facemetadata metadata\"\n",
    "    where_str = \"faces.file_id = imgs.file_id and \" \\\n",
    "                \"faces.face_id = rect.face_id and \" \\\n",
    "                \"faces.face_id = pose.face_id and \" \\\n",
    "                \"faces.face_id = metadata.face_id\"\n",
    "    query_res = exec_sqlite_query(cursor, select_str, from_str, where_str)\n",
    "\n",
    "    # Count total number of images in AFLW dataset\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Count total number of images in AFLW database: \", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    total_num_images = 0\n",
    "    for _ in query_res:\n",
    "        total_num_images += 1\n",
    "    if isVerbose:\n",
    "        print(total_num_images)\n",
    "\n",
    "    # Output file for appending the file paths of not found images\n",
    "    not_found_images_file = 'not_found_images_aflw.txt'\n",
    "    try:\n",
    "        os.remove(not_found_images_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Temporary dataset variables\n",
    "    aflw_dataset_dict = dict()\n",
    "\n",
    "    # Register to dataset_dict\n",
    "    img_cnt = 0\n",
    "    for face_id, path, rectx, recty, rectw, recth, roll, pitch, yaw, gender in query_res:\n",
    "\n",
    "        img_cnt += 1\n",
    "\n",
    "        # Get current image path\n",
    "        img_path = osp.join(dataset_root, 'flickr', path)\n",
    "\n",
    "        # Process current image\n",
    "        if osp.isfile(img_path):\n",
    "            img_w, img_h = get_img_size(img_path)\n",
    "\n",
    "            keypoints = N_LANDMARK * 3 * [0]\n",
    "            pose = [roll, pitch, yaw]\n",
    "            gender = 0 if gender == 'm' else 1\n",
    "\n",
    "            # Register\n",
    "            aflw_dataset_dict[face_id] = {\n",
    "                'face_id': face_id,\n",
    "                'img_path': osp.join('flickr', path),\n",
    "                'width': img_w,\n",
    "                'height': img_h,\n",
    "                'bbox': (rectx, recty, rectw, recth),\n",
    "                'keypoints': keypoints,\n",
    "                'pose': pose,\n",
    "                'gender': gender}\n",
    "\n",
    "        # If current image file does not exist append the not found images filepaths to `not_found_images_file` and\n",
    "        # continue with the next image file.\n",
    "        else:\n",
    "            with open(not_found_images_file, \"a\") as out:\n",
    "                out.write(\"%s\\n\" % img_path)\n",
    "            continue\n",
    "\n",
    "        # Show progress bar\n",
    "        if isVerbose:\n",
    "            progress_updt(\"  \\\\__Populate AFLW dataset dictionary...\", total_num_images, img_cnt)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Update AFLW dataset dictionary with keypoints...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Landmark property\n",
    "    # (Visibility is expressed by lack of the coordinate's row.)\n",
    "    select_str = \"faces.face_id, coords.feature_id, coords.x, coords.y\"\n",
    "    from_str = \"faces, featurecoords coords\"\n",
    "    where_str = \"faces.face_id = coords.face_id\"\n",
    "    query_res = exec_sqlite_query(cursor, select_str, from_str, where_str)\n",
    "\n",
    "    # Register to dataset_dict\n",
    "    invalid_face_ids = list()\n",
    "    for face_id, feature_id, x, y in query_res:\n",
    "        assert (1 <= feature_id <= N_LANDMARK)\n",
    "        if face_id in aflw_dataset_dict:\n",
    "            idx = feature_id - 1\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx] = x\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx + 1] = y\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx + 2] = 1\n",
    "\n",
    "        elif face_id not in invalid_face_ids:\n",
    "            invalid_face_ids.append(face_id)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Close database\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Close the AFLW SQLight database...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    cursor.close()\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Close database\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Convert to COCO format...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    images_list = []\n",
    "    annotations_list = []\n",
    "    iterI = 0\n",
    "    for face_id, face_ann in aflw_dataset_dict.items():\n",
    "        if iterI > 9998:\n",
    "            break\n",
    "        iterI = iterI + 1\n",
    "        \n",
    "        img_dir_num = int(face_ann['img_path'].split(\"/\")[1])\n",
    "        img_file_num = int(re.findall(r'\\d+', face_ann['img_path'].split(\"/\")[-1].split(\".\")[0])[0])\n",
    "        image_id = int(\"%d%05d\" % (img_dir_num, img_file_num))\n",
    "\n",
    "        #save jpg\n",
    "        new_file_name = face_ann['img_path'].split(\"/\")[2]\n",
    "        if(new_file_name.split(\".\")[1] == \"png\"):\n",
    "            new_file_name = new_file_name.split(\".\")[0] + \".jpg\"\n",
    "            \n",
    "        new_path_file = \"allImagesJPG3/\" + new_file_name\n",
    "        copyfile(face_ann['img_path'], new_path_file)\n",
    "        \n",
    "        images_list.append({'id': image_id,\n",
    "                            'file_name': new_file_name,\n",
    "                            'height': face_ann['height'],\n",
    "                            'width': face_ann['width'],\n",
    "                            'date_captured': '',\n",
    "                            'flickr_url': '',\n",
    "                            'license': 1,\n",
    "                            'dataset': 'aflw'})\n",
    "\n",
    "        list_keypoints = []\n",
    "        one_keypoint = []\n",
    "        for value in face_ann['keypoints']:\n",
    "            one_keypoint.append(value)\n",
    "            if len(one_keypoint)==3:\n",
    "                list_keypoints.append(one_keypoint)\n",
    "                one_keypoint= []\n",
    "\n",
    "        #create Array2PNG\n",
    "        w, h = face_ann['width'], face_ann['height']\n",
    "        array2png = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        \n",
    "        segmentations_list = []\n",
    "        for i, keypoint in enumerate(list_keypoints):\n",
    "            v=keypoint[2]\n",
    "            if v == 1: #видно keypoint\n",
    "                x=int(keypoint[0])\n",
    "                y=int(keypoint[1])\n",
    "                segmentation_array = [x,y,x+size_box,y,x+size_box,y+size_box,x,y+size_box]\n",
    "                category_id = i #номер категории[0-20]\n",
    "                id_segmentation = rgb_list_21[i][0] + rgb_list_21[i][1]*256 + rgb_list_21[i][2]*256*256 #мапинг с PNG через RGB: ids=R+G*256+B*256^2\n",
    "                segmentations_list.append({'id': id_segmentation,\n",
    "                                           'category_id': category_id,\n",
    "                                           'area':0,\n",
    "                                           'bbox':[x,y,size_box,size_box],\n",
    "                                           'bbox_mode': \"<BoxMode.XYWH_ABS: 1>\",\n",
    "                                           'iscrowd':0,\n",
    "                                           'segmentation':segmentation_array})\n",
    "                #update array2png\n",
    "                array2png[y:y+size_box, x:x+size_box] = rgb_list_21[i]\n",
    "        \n",
    "        #create PNG\n",
    "        img = Image.fromarray(array2png, 'RGB')\n",
    "        path2origFile = face_ann['img_path']\n",
    "        path2origFile = path2origFile.split(\"/\")\n",
    "        new_path_file_name = dirForNewPNGs + path2origFile[2].split(\".\")[0] + '.png'\n",
    "        new_file_name = path2origFile[2].split(\".\")[0] + '.png'\n",
    "        img.save(new_path_file_name)\n",
    "        \n",
    "        annotations_list.append({'id': face_id,\n",
    "                                 'image_id': image_id,\n",
    "                                 'file_name': new_file_name, #PNG file name,\n",
    "                                 'segments_info': segmentations_list})\n",
    "                                 #'num_keypoints': len(face_ann['keypoints']),\n",
    "                                 #'area': 0,\n",
    "                                 #'iscrowd': 0,\n",
    "                                 #'keypoints': face_ann['keypoints'],\n",
    "                                 #'bbox': face_ann['bbox'],\n",
    "                                 #'category_id': 0})\n",
    "\n",
    "    # Build COCO-like dictionary\n",
    "    dataset_dict = dict()\n",
    "\n",
    "    # =============================== Dataset Info =============================== #\n",
    "    dataset_info = {\n",
    "        'description': 'Annotated Facial Landmarks in the Wild (AFLW)',\n",
    "        'url': 'https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/',\n",
    "        'version': '1.0',\n",
    "        'year': 2011,\n",
    "        'contributor': '',\n",
    "        'date_created': '2011'\n",
    "    }\n",
    "    dataset_dict.update(dataset_info)\n",
    "\n",
    "    # ============================= Dataset Licenses ============================= #\n",
    "    dataset_licenses = {\n",
    "        'licenses': [\n",
    "            {'id': 0,\n",
    "             'url': 'https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/',\n",
    "             'name': 'aflw_license'}\n",
    "        ]\n",
    "    }\n",
    "    dataset_dict.update(dataset_licenses)\n",
    "\n",
    "    # ============================== Dataset Images ============================== #\n",
    "    dataset_images = {'images': images_list}\n",
    "    dataset_dict.update(dataset_images)\n",
    "\n",
    "    # =========================== Dataset Annotations ============================ #\n",
    "    dataset_annotations = {'annotations': annotations_list}\n",
    "    dataset_dict.update(dataset_annotations)\n",
    "\n",
    "    # ============================ Dataset Categories ============================ #\n",
    "    all_keypoints = ['LeftBrowLeftCorner',\n",
    "                     'LeftBrowCenter',\n",
    "                     'LeftBrowRightCorner',\n",
    "                     'RightBrowLeftCorner',\n",
    "                     'RightBrowCenter',\n",
    "                     'RightBrowRightCorner',\n",
    "                     'LeftEyeLeftCorner',\n",
    "                     'LeftEyeCenter',\n",
    "                     'LeftEyeRightCorner',\n",
    "                     'RightEyeLeftCorner',\n",
    "                     'RightEyeCenter',\n",
    "                     'RightEyeRightCorner',\n",
    "                     'LeftEar',\n",
    "                     'NoseLeft',\n",
    "                     'NoseCenter',\n",
    "                     'NoseRight',\n",
    "                     'RightEar',\n",
    "                     'MouthLeftCorner',\n",
    "                     'MouthCenter',\n",
    "                     'MouthRightCorner',\n",
    "                     'ChinCenter']\n",
    "    list_categories = []\n",
    "    for i, category in enumerate(all_keypoints):\n",
    "        list_categories.append({'id': i,\n",
    "                                'name': category,\n",
    "                                'supercategory': category,\n",
    "                                'isthing': 0, #если не отображается сменить на [0,1]\n",
    "                                'color': rgb_list_21[i]})\n",
    "    dataset_categories = {'categories': list_categories}\n",
    "    dataset_dict.update(dataset_categories)\n",
    "\n",
    "    \n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Save dataset dictionary as json file\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Save dataset dictionary as json file...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    with open(jsonFile, 'w') as fp:\n",
    "        json.dump(dataset_dict, fp)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#. Transform AFLW annotations into COCO json format...\n",
      "  \\__Open the AFLW SQLight database...Done!\n",
      "  \\__Count total number of images in AFLW database: 24384\n",
      "  \\__Populate AFLW dataset dictionary...[####################] 100% \n",
      "  \\__Update AFLW dataset dictionary with keypoints...Done!\n",
      "  \\__Close the AFLW SQLight database...Done!\n",
      "  \\__Convert to COCO format...Done!\n",
      "  \\__Save dataset dictionary as json file...Done!\n"
     ]
    }
   ],
   "source": [
    "main_instance()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main2():\n",
    "    # Set up a parser for command line arguments\n",
    "    isVerbose = 1\n",
    "    dataset_root = \"/Users/nikitakorobkov/Downloads/aflw/data\"\n",
    "    json_file = \"aflw_coco12.json\"\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"#. Transform AFLW annotations into COCO json format...\")\n",
    "\n",
    "    # Open the original AFLW annotation (sqlite database)\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Open the AFLW SQLight database...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    conn = sqlite3.connect(osp.join(dataset_root, 'aflw.sqlite'))\n",
    "    cursor = conn.cursor()\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Build sqlite queries\n",
    "    select_str = \"faces.face_id, \" \\\n",
    "                 \"imgs.filepath, \" \\\n",
    "                 \"rect.x, rect.y, \" \\\n",
    "                 \"rect.w, \" \\\n",
    "                 \"rect.h, \" \\\n",
    "                 \"pose.roll, \" \\\n",
    "                 \"pose.pitch, \" \\\n",
    "                 \"pose.yaw, \" \\\n",
    "                 \"metadata.sex\"\n",
    "    from_str = \"faces, \" \\\n",
    "               \"faceimages \" \\\n",
    "               \"imgs, \" \\\n",
    "               \"facerect rect, \" \\\n",
    "               \"facepose pose, \" \\\n",
    "               \"facemetadata metadata\"\n",
    "    where_str = \"faces.file_id = imgs.file_id and \" \\\n",
    "                \"faces.face_id = rect.face_id and \" \\\n",
    "                \"faces.face_id = pose.face_id and \" \\\n",
    "                \"faces.face_id = metadata.face_id\"\n",
    "    query_res = exec_sqlite_query(cursor, select_str, from_str, where_str)\n",
    "\n",
    "    # Count total number of images in AFLW dataset\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Count total number of images in AFLW database: \", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    total_num_images = 0\n",
    "    for _ in query_res:\n",
    "        total_num_images += 1\n",
    "    if isVerbose:\n",
    "        print(total_num_images)\n",
    "\n",
    "    # Output file for appending the file paths of not found images\n",
    "    not_found_images_file = 'not_found_images_aflw.txt'\n",
    "    try:\n",
    "        os.remove(not_found_images_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Temporary dataset variables\n",
    "    aflw_dataset_dict = dict()\n",
    "\n",
    "    # Register to dataset_dict\n",
    "    img_cnt = 0\n",
    "    for face_id, path, rectx, recty, rectw, recth, roll, pitch, yaw, gender in query_res:\n",
    "\n",
    "        img_cnt += 1\n",
    "\n",
    "        # Get current image path\n",
    "        img_path = osp.join(dataset_root, 'flickr', path)\n",
    "\n",
    "        # Process current image\n",
    "        if osp.isfile(img_path):\n",
    "            img_w, img_h = get_img_size(img_path)\n",
    "\n",
    "            keypoints = N_LANDMARK * 3 * [0]\n",
    "            pose = [roll, pitch, yaw]\n",
    "            gender = 0 if gender == 'm' else 1\n",
    "\n",
    "            # Register\n",
    "            aflw_dataset_dict[face_id] = {\n",
    "                'face_id': face_id,\n",
    "                'img_path': osp.join('flickr', path),\n",
    "                'width': img_w,\n",
    "                'height': img_h,\n",
    "                'bbox': (rectx, recty, rectw, recth),\n",
    "                'keypoints': keypoints,\n",
    "                'pose': pose,\n",
    "                'gender': gender}\n",
    "\n",
    "        # If current image file does not exist append the not found images filepaths to `not_found_images_file` and\n",
    "        # continue with the next image file.\n",
    "        else:\n",
    "            with open(not_found_images_file, \"a\") as out:\n",
    "                out.write(\"%s\\n\" % img_path)\n",
    "            continue\n",
    "\n",
    "        # Show progress bar\n",
    "        if isVerbose:\n",
    "            progress_updt(\"  \\\\__Populate AFLW dataset dictionary...\", total_num_images, img_cnt)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Update AFLW dataset dictionary with keypoints...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Landmark property\n",
    "    # (Visibility is expressed by lack of the coordinate's row.)\n",
    "    select_str = \"faces.face_id, coords.feature_id, coords.x, coords.y\"\n",
    "    from_str = \"faces, featurecoords coords\"\n",
    "    where_str = \"faces.face_id = coords.face_id\"\n",
    "    query_res = exec_sqlite_query(cursor, select_str, from_str, where_str)\n",
    "\n",
    "    # Register to dataset_dict\n",
    "    invalid_face_ids = list()\n",
    "    for face_id, feature_id, x, y in query_res:\n",
    "        assert (1 <= feature_id <= N_LANDMARK)\n",
    "        if face_id in aflw_dataset_dict:\n",
    "            idx = feature_id - 1\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx] = x\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx + 1] = y\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx + 2] = 1\n",
    "\n",
    "        elif face_id not in invalid_face_ids:\n",
    "            invalid_face_ids.append(face_id)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Close database\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Close the AFLW SQLight database...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    cursor.close()\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Close database\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Convert to COCO format...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    images_list = []\n",
    "    annotations_list = []\n",
    "    iterI = 0\n",
    "    id_iter = 0\n",
    "    for face_id, face_ann in aflw_dataset_dict.items():\n",
    "        if iterI > 9998:\n",
    "            break\n",
    "        iterI = iterI + 1\n",
    "        \n",
    "        img_dir_num = int(face_ann['img_path'].split(\"/\")[1])\n",
    "        img_file_num = int(re.findall(r'\\d+', face_ann['img_path'].split(\"/\")[-1].split(\".\")[0])[0])\n",
    "        image_id = int(\"%d%05d\" % (img_dir_num, img_file_num))\n",
    "\n",
    "        #save jpg\n",
    "        new_file_name = face_ann['img_path'].split(\"/\")[2]\n",
    "        if(new_file_name.split(\".\")[1] == \"png\"):\n",
    "            new_file_name = new_file_name.split(\".\")[0] + \".jpg\"\n",
    "            \n",
    "        new_path_file = \"allImagesJPG3/\" + new_file_name\n",
    "        #copyfile(face_ann['img_path'], new_path_file)\n",
    "        \n",
    "        list_keypoints = []\n",
    "        one_keypoint = []\n",
    "        for value in face_ann['keypoints']:\n",
    "            one_keypoint.append(value)\n",
    "            if len(one_keypoint)==3:\n",
    "                list_keypoints.append(one_keypoint)\n",
    "                one_keypoint= []\n",
    "\n",
    "        images_list.append({'id': image_id,\n",
    "                            'file_name': new_file_name,\n",
    "                            'height': face_ann['height'],\n",
    "                            'width': face_ann['width'],\n",
    "                            'date_captured': '',\n",
    "                            'flickr_url': '',\n",
    "                            'license': 1,\n",
    "                            'dataset': 'aflw'})\n",
    "        \n",
    "        segmentations_list = []\n",
    "        for i, keypoint in enumerate(list_keypoints):\n",
    "            v=keypoint[2]\n",
    "            if v == 1: #видно keypoint\n",
    "                x=int(keypoint[0])\n",
    "                y=int(keypoint[1])\n",
    "                segmentation_array = [[x,y,x+size_box,y,x+size_box,y+size_box,x,y+size_box]]\n",
    "                category_id = i #номер категории[0-20]\n",
    "            \n",
    "                id_segmentation = rgb_list_21[i][0] + rgb_list_21[i][1]*256 + rgb_list_21[i][2]*256*256 #мапинг с PNG через RGB: ids=R+G*256+B*256^2\n",
    "                annotations_list.append({'id': id_iter+500000,\n",
    "                                         'image_id': image_id,\n",
    "                                         'category_id': category_id,\n",
    "                                         'area':0,\n",
    "                                         'bbox':[x,y,size_box,size_box],\n",
    "                                         'iscrowd':0,\n",
    "                                         'segmentation':segmentation_array})\n",
    "                id_iter = id_iter+1\n",
    "        \n",
    "        x1 = face_ann['bbox'][0]\n",
    "        y1 = face_ann['bbox'][1]\n",
    "        x2 = face_ann['bbox'][0] + face_ann['bbox'][2]\n",
    "        y2 = face_ann['bbox'][1] + face_ann['bbox'][3]\n",
    "        \n",
    "        annotations_list.append({'id': face_id,\n",
    "                                 'image_id': image_id,\n",
    "                                 'segmentation': [[x1,y1,x2,y1,x2,y2,x1,y2]],\n",
    "                                 #'num_keypoints': len(face_ann['keypoints']),\n",
    "                                 'area': 0,\n",
    "                                 'iscrowd': 0,\n",
    "                                 #'keypoints': face_ann['keypoints'],\n",
    "                                 'bbox': face_ann['bbox'],\n",
    "                                 'category_id': 21}) #21 - face\n",
    "\n",
    "    # Build COCO-like dictionary\n",
    "    dataset_dict = dict()\n",
    "\n",
    "    # =============================== Dataset Info =============================== #\n",
    "    dataset_info = {\n",
    "        'description': 'Annotated Facial Landmarks in the Wild (AFLW)',\n",
    "        'url': 'https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/',\n",
    "        'version': '1.0',\n",
    "        'year': 2011,\n",
    "        'contributor': '',\n",
    "        'date_created': '2011'\n",
    "    }\n",
    "    dataset_dict.update(dataset_info)\n",
    "\n",
    "    # ============================= Dataset Licenses ============================= #\n",
    "    dataset_licenses = {\n",
    "        'licenses': [\n",
    "            {'id': 0,\n",
    "             'url': 'https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/',\n",
    "             'name': 'aflw_license'}\n",
    "        ]\n",
    "    }\n",
    "    dataset_dict.update(dataset_licenses)\n",
    "\n",
    "    # ============================== Dataset Images ============================== #\n",
    "    dataset_images = {'images': images_list}\n",
    "    dataset_dict.update(dataset_images)\n",
    "\n",
    "    # =========================== Dataset Annotations ============================ #\n",
    "    dataset_annotations = {'annotations': annotations_list}\n",
    "    dataset_dict.update(dataset_annotations)\n",
    "\n",
    "    # ============================ Dataset Categories ============================ #\n",
    "    all_keypoints = ['LeftBrowLeftCorner',\n",
    "                     'LeftBrowCenter',\n",
    "                     'LeftBrowRightCorner',\n",
    "                     'RightBrowLeftCorner',\n",
    "                     'RightBrowCenter',\n",
    "                     'RightBrowRightCorner',\n",
    "                     'LeftEyeLeftCorner',\n",
    "                     'LeftEyeCenter',\n",
    "                     'LeftEyeRightCorner',\n",
    "                     'RightEyeLeftCorner',\n",
    "                     'RightEyeCenter',\n",
    "                     'RightEyeRightCorner',\n",
    "                     'LeftEar',\n",
    "                     'NoseLeft',\n",
    "                     'NoseCenter',\n",
    "                     'NoseRight',\n",
    "                     'RightEar',\n",
    "                     'MouthLeftCorner',\n",
    "                     'MouthCenter',\n",
    "                     'MouthRightCorner',\n",
    "                     'ChinCenter',\n",
    "                     'Face']\n",
    "    list_categories = []\n",
    "    for i, category in enumerate(all_keypoints):\n",
    "        list_categories.append({'id': i,\n",
    "                                'name': category,\n",
    "                                'supercategory': category,\n",
    "                                'isthing': 0, #если не отображается сменить на [0,1]\n",
    "                                'color': rgb_list_21[i]})\n",
    "    dataset_categories = {'categories': list_categories}\n",
    "    dataset_dict.update(dataset_categories)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Save dataset dictionary as json file\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Save dataset dictionary as json file...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    with open(json_file, 'w') as fp:\n",
    "        json.dump(dataset_dict, fp)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#. Transform AFLW annotations into COCO json format...\n",
      "  \\__Open the AFLW SQLight database...Done!\n",
      "  \\__Count total number of images in AFLW database: 24384\n",
      "  \\__Populate AFLW dataset dictionary...[###############-----] 75% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \\__Populate AFLW dataset dictionary...[##################--] 91% "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  \\__Populate AFLW dataset dictionary...[####################] 100% \n",
      "  \\__Update AFLW dataset dictionary with keypoints...Done!\n",
      "  \\__Close the AFLW SQLight database...Done!\n",
      "  \\__Convert to COCO format...Done!\n",
      "  \\__Save dataset dictionary as json file...Done!\n"
     ]
    }
   ],
   "source": [
    "main2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_box = 6\n",
    "\n",
    "def main_instPanoptic():\n",
    "    # Set up a parser for command line arguments\n",
    "    isVerbose = 1\n",
    "    dataset_root = \"/Users/nikitakorobkov/Downloads/aflw/data\"\n",
    "    jsonFile = \"panoptic_aflw4coco.json\"\n",
    "    dirForNewPNGs = \"allImagesPNG5/\"\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"#. Transform AFLW annotations into COCO json format...\")\n",
    "\n",
    "    # Open the original AFLW annotation (sqlite database)\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Open the AFLW SQLight database...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    conn = sqlite3.connect(osp.join(dataset_root, 'aflw.sqlite'))\n",
    "    cursor = conn.cursor()\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Build sqlite queries\n",
    "    select_str = \"faces.face_id, \" \\\n",
    "                 \"imgs.filepath, \" \\\n",
    "                 \"rect.x, rect.y, \" \\\n",
    "                 \"rect.w, \" \\\n",
    "                 \"rect.h, \" \\\n",
    "                 \"pose.roll, \" \\\n",
    "                 \"pose.pitch, \" \\\n",
    "                 \"pose.yaw, \" \\\n",
    "                 \"metadata.sex\"\n",
    "    from_str = \"faces, \" \\\n",
    "               \"faceimages \" \\\n",
    "               \"imgs, \" \\\n",
    "               \"facerect rect, \" \\\n",
    "               \"facepose pose, \" \\\n",
    "               \"facemetadata metadata\"\n",
    "    where_str = \"faces.file_id = imgs.file_id and \" \\\n",
    "                \"faces.face_id = rect.face_id and \" \\\n",
    "                \"faces.face_id = pose.face_id and \" \\\n",
    "                \"faces.face_id = metadata.face_id\"\n",
    "    query_res = exec_sqlite_query(cursor, select_str, from_str, where_str)\n",
    "\n",
    "    # Count total number of images in AFLW dataset\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Count total number of images in AFLW database: \", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    total_num_images = 0\n",
    "    for _ in query_res:\n",
    "        total_num_images += 1\n",
    "    if isVerbose:\n",
    "        print(total_num_images)\n",
    "\n",
    "    # Output file for appending the file paths of not found images\n",
    "    not_found_images_file = 'not_found_images_aflw.txt'\n",
    "    try:\n",
    "        os.remove(not_found_images_file)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    # Temporary dataset variables\n",
    "    aflw_dataset_dict = dict()\n",
    "\n",
    "    # Register to dataset_dict\n",
    "    img_cnt = 0\n",
    "    for face_id, path, rectx, recty, rectw, recth, roll, pitch, yaw, gender in query_res:\n",
    "\n",
    "        img_cnt += 1\n",
    "\n",
    "        # Get current image path\n",
    "        img_path = osp.join(dataset_root, 'flickr', path)\n",
    "\n",
    "        # Process current image\n",
    "        if osp.isfile(img_path):\n",
    "            img_w, img_h = get_img_size(img_path)\n",
    "\n",
    "            keypoints = N_LANDMARK * 3 * [0]\n",
    "            pose = [roll, pitch, yaw]\n",
    "            gender = 0 if gender == 'm' else 1\n",
    "\n",
    "            # Register\n",
    "            aflw_dataset_dict[face_id] = {\n",
    "                'face_id': face_id,\n",
    "                'img_path': osp.join('flickr', path),\n",
    "                'width': img_w,\n",
    "                'height': img_h,\n",
    "                'bbox': (rectx, recty, rectw, recth),\n",
    "                'keypoints': keypoints,\n",
    "                'pose': pose,\n",
    "                'gender': gender}\n",
    "\n",
    "        # If current image file does not exist append the not found images filepaths to `not_found_images_file` and\n",
    "        # continue with the next image file.\n",
    "        else:\n",
    "            with open(not_found_images_file, \"a\") as out:\n",
    "                out.write(\"%s\\n\" % img_path)\n",
    "            continue\n",
    "\n",
    "        # Show progress bar\n",
    "        if isVerbose:\n",
    "            progress_updt(\"  \\\\__Populate AFLW dataset dictionary...\", total_num_images, img_cnt)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Update AFLW dataset dictionary with keypoints...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    # Landmark property\n",
    "    # (Visibility is expressed by lack of the coordinate's row.)\n",
    "    select_str = \"faces.face_id, coords.feature_id, coords.x, coords.y\"\n",
    "    from_str = \"faces, featurecoords coords\"\n",
    "    where_str = \"faces.face_id = coords.face_id\"\n",
    "    query_res = exec_sqlite_query(cursor, select_str, from_str, where_str)\n",
    "\n",
    "    # Register to dataset_dict\n",
    "    invalid_face_ids = list()\n",
    "    for face_id, feature_id, x, y in query_res:\n",
    "        assert (1 <= feature_id <= N_LANDMARK)\n",
    "        if face_id in aflw_dataset_dict:\n",
    "            idx = feature_id - 1\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx] = x\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx + 1] = y\n",
    "            aflw_dataset_dict[face_id]['keypoints'][3 * idx + 2] = 1\n",
    "\n",
    "        elif face_id not in invalid_face_ids:\n",
    "            invalid_face_ids.append(face_id)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Close database\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Close the AFLW SQLight database...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "    cursor.close()\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Close database\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Convert to COCO format...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    images_list = []\n",
    "    annotations_list = []\n",
    "    iterI = 0\n",
    "    for face_id, face_ann in aflw_dataset_dict.items():\n",
    "        if iterI > 9998:\n",
    "            break\n",
    "        iterI = iterI + 1\n",
    "        \n",
    "        img_dir_num = int(face_ann['img_path'].split(\"/\")[1])\n",
    "        img_file_num = int(re.findall(r'\\d+', face_ann['img_path'].split(\"/\")[-1].split(\".\")[0])[0])\n",
    "        image_id = int(\"%d%05d\" % (img_dir_num, img_file_num))\n",
    "\n",
    "        #save jpg\n",
    "        new_path_file = dirForNewPNGs + face_ann['img_path'].split(\"/\")[2]\n",
    "\n",
    "        images_list.append({'id': image_id,\n",
    "                            'file_name': new_path_file,\n",
    "                            'height': face_ann['height'],\n",
    "                            'width': face_ann['width'],\n",
    "                            'date_captured': '',\n",
    "                            'flickr_url': '',\n",
    "                            'license': 1,\n",
    "                            'dataset': 'aflw'})\n",
    "\n",
    "        list_keypoints = []\n",
    "        one_keypoint = []\n",
    "        for value in face_ann['keypoints']:\n",
    "            one_keypoint.append(value)\n",
    "            if len(one_keypoint)==3:\n",
    "                list_keypoints.append(one_keypoint)\n",
    "                one_keypoint= []\n",
    "\n",
    "        #create Array2PNG\n",
    "        w, h = face_ann['width'], face_ann['height']\n",
    "        array2png = np.zeros((h, w, 3), dtype=np.uint8)\n",
    "        \n",
    "        segmentations_list = []\n",
    "        for i, keypoint in enumerate(list_keypoints):\n",
    "            v=keypoint[2]\n",
    "            if v == 1: #видно keypoint\n",
    "                x=int(keypoint[0])\n",
    "                y=int(keypoint[1])\n",
    "                segmentation_array = [[x,y,x+size_box,y,x+size_box,y+size_box,x,y+size_box]]\n",
    "                category_id = i #номер категории[0-20]\n",
    "                id_segmentation = rgb_list_21[i][0] + rgb_list_21[i][1]*256 + rgb_list_21[i][2]*256*256 #мапинг с PNG через RGB: ids=R+G*256+B*256^2\n",
    "                segmentations_list.append({'id': id_segmentation,\n",
    "                                           'category_id': category_id,\n",
    "                                           'area':0,\n",
    "                                           'bbox':[x,y,size_box,size_box],\n",
    "                                           'bbox_mode': \"<BoxMode.XYWH_ABS: 1>\",\n",
    "                                           'iscrowd':0,\n",
    "                                           'segmentation':segmentation_array})\n",
    "                #update array2png\n",
    "                array2png[y:y+size_box, x:x+size_box] = rgb_list_21[i]\n",
    "        \n",
    "        #create PNG\n",
    "        img = Image.fromarray(array2png, 'RGB')\n",
    "        path2origFile = face_ann['img_path']\n",
    "        path2origFile = path2origFile.split(\"/\")\n",
    "        new_path_file_name = dirForPathPNGs + path2origFile[2].split(\".\")[0] + '.png'\n",
    "        new_file_name = path2origFile[2].split(\".\")[0] + '.png'\n",
    "        img.save(new_path_file_name)\n",
    "        \n",
    "        annotations_list.append({'image_id': image_id,\n",
    "                                 'file_name': new_file_name, #PNG file name,\n",
    "                                 'segments_info': segmentations_list})\n",
    "                                 #'num_keypoints': len(face_ann['keypoints']),\n",
    "                                 #'area': 0,\n",
    "                                 #'iscrowd': 0,\n",
    "                                 #'keypoints': face_ann['keypoints'],\n",
    "                                 #'bbox': face_ann['bbox'],\n",
    "                                 #'category_id': 0})\n",
    "\n",
    "    # Build COCO-like dictionary\n",
    "    dataset_dict = dict()\n",
    "\n",
    "    # =============================== Dataset Info =============================== #\n",
    "    dataset_info = {\n",
    "        'description': 'Annotated Facial Landmarks in the Wild (AFLW)',\n",
    "        'url': 'https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/',\n",
    "        'version': '1.0',\n",
    "        'year': 2011,\n",
    "        'contributor': '',\n",
    "        'date_created': '2011'\n",
    "    }\n",
    "    dataset_dict.update(dataset_info)\n",
    "\n",
    "    # ============================= Dataset Licenses ============================= #\n",
    "    dataset_licenses = {\n",
    "        'licenses': [\n",
    "            {'id': 0,\n",
    "             'url': 'https://www.tugraz.at/institute/icg/research/team-bischof/lrs/downloads/aflw/',\n",
    "             'name': 'aflw_license'}\n",
    "        ]\n",
    "    }\n",
    "    dataset_dict.update(dataset_licenses)\n",
    "\n",
    "    # ============================== Dataset Images ============================== #\n",
    "    dataset_images = {'images': images_list}\n",
    "    dataset_dict.update(dataset_images)\n",
    "\n",
    "    # =========================== Dataset Annotations ============================ #\n",
    "    dataset_annotations = {'annotation': annotations_list}\n",
    "    dataset_dict.update(dataset_annotations)\n",
    "\n",
    "    # ============================ Dataset Categories ============================ #\n",
    "    all_keypoints = ['LeftBrowLeftCorner',\n",
    "                     'LeftBrowCenter',\n",
    "                     'LeftBrowRightCorner',\n",
    "                     'RightBrowLeftCorner',\n",
    "                     'RightBrowCenter',\n",
    "                     'RightBrowRightCorner',\n",
    "                     'LeftEyeLeftCorner',\n",
    "                     'LeftEyeCenter',\n",
    "                     'LeftEyeRightCorner',\n",
    "                     'RightEyeLeftCorner',\n",
    "                     'RightEyeCenter',\n",
    "                     'RightEyeRightCorner',\n",
    "                     'LeftEar',\n",
    "                     'NoseLeft',\n",
    "                     'NoseCenter',\n",
    "                     'NoseRight',\n",
    "                     'RightEar',\n",
    "                     'MouthLeftCorner',\n",
    "                     'MouthCenter',\n",
    "                     'MouthRightCorner',\n",
    "                     'ChinCenter']\n",
    "    list_categories = []\n",
    "    for i, category in enumerate(all_keypoints):\n",
    "        list_categories.append({'id': i,\n",
    "                                'name': category,\n",
    "                                'supercategory': category,\n",
    "                                'isthing': 0, #если не отображается сменить на [0,1]\n",
    "                                'color': rgb_list_21[i]})\n",
    "    dataset_categories = {'categories': list_categories}\n",
    "    dataset_dict.update(dataset_categories)\n",
    "\n",
    "    \n",
    "    if isVerbose:\n",
    "        print(\"Done!\")\n",
    "\n",
    "    # Save dataset dictionary as json file\n",
    "    if isVerbose:\n",
    "        print(\"  \\\\__Save dataset dictionary as json file...\", end=\"\")\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    with open(jsonFile, 'w') as fp:\n",
    "        json.dump(dataset_dict, fp)\n",
    "\n",
    "    if isVerbose:\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
